% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calc_mutual_info.R
\name{calc_mutual_info}
\alias{calc_mutual_info}
\title{Calculate mutual information from two vectors}
\usage{
calc_mutual_info(a_vec, b_vec, log_base = 2)
}
\arguments{
\item{a_vec}{vector of observations (can be logical, factor, character, or integer).}

\item{b_vec}{paired observations for second variable.}

\item{log_base}{Base of logarithm used. \code{2} will return entropy in bits.}
}
\value{
mutual information between the two vectors.
}
\description{
Calculates mutual information from two vectors of observations for
non-continuous data. There is no parametric assumption here but if the
number of unique values for either vector is high smoothing (see other
package functions) may improve performance.
}
\details{
Note that mutual information is symmetric so \code{calc_mutual_info(a,b) == calc_mutual_info(b,a)}.
}
\examples{


}
